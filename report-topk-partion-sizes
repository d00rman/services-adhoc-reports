#!/usr/bin/env python

# TODO: pylint (120 char line limit) 
# TODO: validate presence of jsonschema and requests modules?
# TODO: make it email a report
# TODO: Comments, docstrings
# TODO: Licensing

import argparse
import datetime
import jsonschema
import operator
import re
import requests


class Logstash(object):
    page_size = 200

    def __init__(self, index, host, port=9200):
        self.index = index
        self.host = host
        self.port = port

    def __url(self):
        return "http://{0.host}:{0.port}/{0.index}/_search".format(self)

    def search(self, query):
        if not isinstance(query, dict):
            raise RuntimeError("invalid argument; query must be a dictionary")
        query["from"] = 0
        query["size"] = Logstash.page_size
        res = Results(requests.get(self.__url(), json=query))
        for hit in res.hits:
            yield hit
        for _ in range((res.total / Logstash.page_size) + 1):
            query["from"] += len(res.hits)
            res = Results(requests.get(self.__url(), json=query))
            for hit in res.hits:
                yield hit

# TODO: Do JSON schema validation
# TODO: Read-only properties?
# XXX: Rename to Page, LogstashPage?
class Results(object):
    response_schema = {
        "type": "object",
        "properties": {
            "timed_out": {
                "type": "boolean"   # ???
            },
            "hits": {
                "type": "object",
                "properties": {
                    "hits": {
                        "type": "array"
                    }
                }
            }
        },
        "required": ["timed_out", "hits"]
    }
    
    def __init__(self, response):
        assert response.status_code == 200
        Results.validate_response(response)

        json_response = response.json()
        assert json_response.has_key("timed_out")
        assert not json_response["timed_out"]
        assert json_response.has_key("hits")
        assert json_response["hits"].has_key("total")
        assert json_response["hits"].has_key("hits")
        assert isinstance(json_response["hits"]["hits"], list)

        self.total = json_response["hits"]["total"]
        self.hits = json_response["hits"]["hits"]

    @classmethod
    def validate_response(cls, response):
        jsonschema.validate(response.json(), Results.response_schema)


def search_query(cluster="eqiad"):
    return {
        "query": {
            "bool": {
                "must": [
                    {"match": {"logger_name": "org.apache.cassandra.io.sstable.format.big.BigTableWriter"}},
                    {"match": {"cluster": cluster}},
                    {"match_phrase": {"message": "Writing large partition"}}
                ]
            }
        },
        "_source": ["message"]
    }

def index_names(days=7):
    name = lambda dt: dt.strftime("logstash-%Y.%m.%d")
    now = datetime.datetime.now()
    yield name(now)
    for i in range(days):
        yield name(now - datetime.timedelta(days=i+1))

def strfsize(num, suffix='B'):
    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
        if abs(num) < 1024.0:
            return "%.1f%s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f%s%s" % (num, 'Yi', suffix)

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Generate an email report of the widest Cassandra partitions.")
    parser.add_argument(
        "-e", "--email", metavar="ADDRESS", required=True, help="Email address to send report to")
    parser.add_argument(
        "-H", "--logstash-host", metavar="HOST", default="logstash1001.eqiad.wmnet", help="Logstash hostname or address to search")
    parser.add_argument(
        "-p", "--logstash-port", metavar="PORT", type=int, default=9200, help="Logstash port number")
    parser.add_argument(
        "-k", "--top", metavar="N", type=int, default=50, help="Number of results to report")
    return parser.parse_args()

def main():
    args = parse_arguments()
    host = args.logstash_host
    port = args.logstash_port
    k = args.top
    count = 0
    log = re.compile(r"Writing large partition (?P<partition>.+) \((?P<bytes>[\d]+) bytes\)")
    for index in index_names():
        results = {}
        for hit in Logstash(index, host, port).search(search_query()):
            count += 1
            message = hit["_source"]["message"]
            match = log.match(message)
            if not match:
                continue   # XXX: Maybe warn or something; This shouldn't happen
            partition = match.group("partition")
            size = match.group("bytes")
            if results.get(partition, 0) < size:
                results[partition] = int(size)
        values = sorted(results.items(), key=operator.itemgetter(1), reverse=True)
        print "Processed {} log entries.".format(count)
        print "{} unique partitions.".format(len(values))
        for i in range(k):
            print u"{}: {}".format(strfsize(values[i][1]), values[i][0])


if __name__ == "__main__":
    main()
